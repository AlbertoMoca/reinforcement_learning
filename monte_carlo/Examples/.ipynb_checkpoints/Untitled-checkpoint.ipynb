{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f227a0eb-8669-445c-9dd6-192b335d008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "if \"../../\" not in sys.path:\n",
    "  sys.path.append(\"../../\") \n",
    "from lib.envs.blackjack import BlackjackEnv\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7831adec-7fa5-4147-8c4a-66915bd18480",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a449d34-d6c6-4f7f-b60f-66d2edb71ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.is_training = True\n",
    "        self.epsilon = .1\n",
    "        self.env = BlackjackEnv()\n",
    "        # Set of state\n",
    "        self.set_of_states = {\n",
    "            \"dealer_showing\": (1,10),\n",
    "            \"player_sum\": (12,21),\n",
    "            \"usable_ace\": (0,1),\n",
    "        }\n",
    "        # Value function\n",
    "        self.V = np.zeros((   \n",
    "            self.set_of_states[\"dealer_showing\"][1]-self.set_of_states[\"dealer_showing\"][0]+1,\n",
    "            self.set_of_states[\"player_sum\"][1]-self.set_of_states[\"player_sum\"][0]+1,\n",
    "            self.set_of_states[\"usable_ace\"][1]-self.set_of_states[\"usable_ace\"][0]+1))\n",
    "        \n",
    "        # State-Action function\n",
    "        self.Q = np.random.random((   \n",
    "            self.set_of_states[\"dealer_showing\"][1]-self.set_of_states[\"dealer_showing\"][0]+1,\n",
    "            self.set_of_states[\"player_sum\"][1]-self.set_of_states[\"player_sum\"][0]+1,\n",
    "            self.set_of_states[\"usable_ace\"][1]-self.set_of_states[\"usable_ace\"][0]+1,\n",
    "            2\n",
    "        ))+2\n",
    "        \n",
    "        self.N = np.zeros(self.Q.shape)\n",
    "        \n",
    "        self.policy = np.random.choice([0,1],(   \n",
    "            self.set_of_states[\"dealer_showing\"][1]-self.set_of_states[\"dealer_showing\"][0]+1,\n",
    "            self.set_of_states[\"player_sum\"][1]-self.set_of_states[\"player_sum\"][0]+1,\n",
    "            self.set_of_states[\"usable_ace\"][1]-self.set_of_states[\"usable_ace\"][0]+1))\n",
    "        #self.policy[:,:,:] = 1\n",
    "        #self.policy[:,-2:,:] = 0\n",
    "        self.gamma = 1\n",
    "        \n",
    "        \n",
    "        self.Returns = []\n",
    "        \n",
    "        for i in range(self.Q.shape[0]):\n",
    "            a = []\n",
    "            for j in range(self.Q.shape[1]):\n",
    "                b = []\n",
    "                for k in range(self.Q.shape[2]):\n",
    "                    c = []\n",
    "                    for l in range(self.Q.shape[3]):\n",
    "                        c.append([])\n",
    "                    b.append(c)\n",
    "                a.append(b)\n",
    "            self.Returns.append(a)\n",
    "            \n",
    "        self.Returns_V = []\n",
    "        \n",
    "        for i in range(self.V.shape[0]):\n",
    "            a = []\n",
    "            for j in range(self.V.shape[1]):\n",
    "                b = []\n",
    "                for k in range(self.V.shape[2]):\n",
    "                    b.append([])\n",
    "                a.append(b)\n",
    "            self.Returns_V.append(a)\n",
    "        \n",
    "        \n",
    "    def iterate_policy(self, episodes):\n",
    "        \n",
    "        for i in tqdm(range(episodes)):\n",
    "            \n",
    "            states, rewards, actions = self.generate_episode()\n",
    "            states = states[:-1]            \n",
    "            \n",
    "            G = 0\n",
    "            for t in range(0,len(states))[::-1]:\n",
    "                G = self.gamma * G + rewards[t]\n",
    "                \n",
    "                #print(f\"G:{G}\")\n",
    "                case_pos = (states[t][0]-self.set_of_states[\"dealer_showing\"][0],states[t][1]-self.set_of_states[\"player_sum\"][0],states[t][2]-self.set_of_states[\"usable_ace\"][0])\n",
    "                \n",
    "                self.N[case_pos[0]][case_pos[1]][case_pos[2]][int(actions[t])] += 1\n",
    "                self.Q[case_pos[0]][case_pos[1]][case_pos[2]][int(actions[t])] += (G-self.Q[case_pos[0]][case_pos[1]][case_pos[2]][int(actions[t])]) /self.N[case_pos[0]][case_pos[1]][case_pos[2]][int(actions[t])]\n",
    "                self.policy = np.argmax(self.Q,axis=-1)\n",
    "                \n",
    "                #self.Returns[case_pos[0]][case_pos[1]][case_pos[2]][int(actions[t])].append(G)\n",
    "                #self.Q[case_pos[0]][case_pos[1]][case_pos[2]][int(actions[t])] = np.mean(self.Returns[case_pos[0]][case_pos[1]][case_pos[2]][int(actions[t])])\n",
    "                #self.policy = np.argmax(self.Q,axis=-1)\n",
    "            \n",
    "            \n",
    "    def strategy(self, observation):\n",
    "        state = [observation[1],observation[0],int(observation[2])]\n",
    "        case_pos = (state[0]-self.set_of_states[\"dealer_showing\"][0],state[1]-self.set_of_states[\"player_sum\"][0],state[2]-self.set_of_states[\"usable_ace\"][0])\n",
    "        \n",
    "        if self.is_training:\n",
    "            return(np.random.choice([0,1,self.policy[case_pos[0]][case_pos[1]][case_pos[2]]],p=[self.epsilon/2,self.epsilon/2,1-self.epsilon]))\n",
    "        else:\n",
    "            return(self.policy[case_pos[0]][case_pos[1]][case_pos[2]])\n",
    "\n",
    "    def print_observation(self, observation):\n",
    "        score, dealer_score, usable_ace = observation\n",
    "        print(\"Player Score: {} (Usable Ace: {}), Dealer Score: {}\".format(\n",
    "              score, usable_ace, dealer_score))\n",
    "        \n",
    "    def aproximateV(self,episodes):\n",
    "        for i in tqdm(range(episodes)):\n",
    "            states, rewards, actions = self.generate_episode()\n",
    "            #print(f\"States: {states}\")\n",
    "            #print(f\"Rewards: {rewards}\")\n",
    "            #print(f\"Actions: {actions}\")\n",
    "            states = states[:-1]\n",
    "            G = 0\n",
    "            for t in range(0,len(states))[::-1]:\n",
    "                \n",
    "                G = self.gamma * G + rewards[t]\n",
    "                #print(states[t])\n",
    "                case_pos = (states[t][0]-self.set_of_states[\"dealer_showing\"][0],states[t][1]-self.set_of_states[\"player_sum\"][0],states[t][2]-self.set_of_states[\"usable_ace\"][0])\n",
    "                \n",
    "                #self.NV[case_pos[0]][case_pos[1]][case_pos[2]] += 1\n",
    "                #self.V[case_pos[0]][case_pos[1]][case_pos[2]] += (G-self.V[case_pos[0]][case_pos[1]][case_pos[2]]) /self.NV[case_pos[0]][case_pos[1]][case_pos[2]]\n",
    "                self.Returns_V[states[t][0]-self.set_of_states[\"dealer_showing\"][0]][states[t][1]-self.set_of_states[\"player_sum\"][0]][states[t][2]-self.set_of_states[\"usable_ace\"][0]].append(G)\n",
    "                self.V[states[t][0]-self.set_of_states[\"dealer_showing\"][0]][states[t][1]-self.set_of_states[\"player_sum\"][0]][states[t][2]-self.set_of_states[\"usable_ace\"][0]] = np.mean(self.Returns_V[states[t][0]-self.set_of_states[\"dealer_showing\"][0]][states[t][1]-self.set_of_states[\"player_sum\"][0]][states[t][2]-self.set_of_states[\"usable_ace\"][0]])\n",
    "            \n",
    "            \n",
    "            \n",
    "    def generate_episode(self):\n",
    "        observation = self.env.reset()\n",
    "        #self.print_observation(observation)\n",
    "        states, rewards, actions = [], [], []\n",
    "        states.append([observation[1],observation[0],int(observation[2])])\n",
    "        \n",
    "        for t in range(100):\n",
    "            \n",
    "            action = self.strategy(observation)\n",
    "            actions.append(action)\n",
    "            #print(f\"action: {action}\")\n",
    "            \n",
    "            observation, reward, done, _ = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            states.append([observation[1],observation[0],int(observation[2])])\n",
    "            \n",
    "            #self.print_observation(observation)\n",
    "            if done:\n",
    "                \n",
    "                #print(\"Game end. Reward: {}\\n\".format(float(reward)))\n",
    "                break\n",
    "        return states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3844c765-8dd7-443f-a5cb-14649d0e9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 stay\n",
    "#1 hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020c9be-6ecd-41f3-afc5-a33f6745e446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d246ffd7daf464a814801e4286bd4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Model()\n",
    "m.iterate_policy(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea48a3db-9434-4939-b60b-fae56e9b1460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5576f23e874909ae2d030d0d7a467c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "m.aproximateV(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2c035a0-1ef1-4477-ad66-7d8150a50792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6224e805f292464b9d1dc0862466f24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "#fig.set_size_inches(10, 10)\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(12,22)\n",
    "Y = np.arange(1,11)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(X, Y, m.V[:,:,0], linewidth=0, antialiased=False,cmap=cm.plasma)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(X, Y, m.V[:,:,1], linewidth=0, antialiased=False,cmap=cm.plasma)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf71df5d-ddc0-43e1-86e1-0e8a9ad7f087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb6c7b74d544250b9ae7b3ae95735a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "#fig.set_size_inches(10, 10)\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(12,22)\n",
    "Y = np.arange(1,11)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(X, Y, m.policy[:,:,0], linewidth=0, antialiased=False,cmap=cm.plasma)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(X, Y, m.policy[:,:,1], linewidth=0, antialiased=False,cmap=cm.plasma)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5eb846af-878d-4bf5-88b2-e39260572442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd931dc88ef44dfebc2dd34d710507f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(m.policy[:,:,0],cmap=\"gray\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(m.policy[:,:,1],cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f3db8-2d42-475e-bfa7-f4f7dce04c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
