{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73775d21-1502-4841-aafd-5f6bdda414ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "if \"../../\" not in sys.path:\n",
    "  sys.path.append(\"../../\") \n",
    "from lib.envs.blackjack import BlackjackEnv\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9034d30b-009d-4fef-adf3-4581742cf0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe169a01-d27e-457a-ab8e-e4c635e0ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.env = BlackjackEnv()\n",
    "        # Set of state\n",
    "        self.set_of_states = {\n",
    "            \"dealer_showing\": (1,10),\n",
    "            \"player_sum\": (12,21),\n",
    "            \"usable_ace\": (0,1),\n",
    "        }\n",
    "        # Value function\n",
    "        self.V = np.zeros((   \n",
    "            self.set_of_states[\"dealer_showing\"][1]-self.set_of_states[\"dealer_showing\"][0]+1,\n",
    "            self.set_of_states[\"player_sum\"][1]-self.set_of_states[\"player_sum\"][0]+1,\n",
    "            self.set_of_states[\"usable_ace\"][1]-self.set_of_states[\"usable_ace\"][0]+1))\n",
    "        \n",
    "        \n",
    "        self.gamma = 1\n",
    "        \n",
    "        self.NV = np.zeros(self.V.shape)\n",
    "        \n",
    "            \n",
    "    def strategy(self, observation):\n",
    "        score, dealer_score, usable_ace = observation\n",
    "        # Stick (action 0) if the score is > 20, hit (action 1) otherwise\n",
    "        return 0 if score >= 20 else 1\n",
    "\n",
    "    def print_observation(self, observation):\n",
    "        score, dealer_score, usable_ace = observation\n",
    "        print(\"Player Score: {} (Usable Ace: {}), Dealer Score: {}\".format(\n",
    "              score, usable_ace, dealer_score))\n",
    "    def aproximateV(self,episodes):\n",
    "        for i in tqdm(range(episodes)):\n",
    "            states, rewards, actions = self.generate_episode()\n",
    "            #print(f\"States: {states}\")\n",
    "            #print(f\"Rewards: {rewards}\")\n",
    "            #print(f\"Actions: {actions}\")\n",
    "            states = states[:-1]\n",
    "            G = 0\n",
    "            for t in range(0,len(states))[::-1]:\n",
    "                \n",
    "                G = self.gamma * G + rewards[t]\n",
    "                #print(states[t])\n",
    "                case_pos = (states[t][0]-self.set_of_states[\"dealer_showing\"][0],states[t][1]-self.set_of_states[\"player_sum\"][0],states[t][2]-self.set_of_states[\"usable_ace\"][0])\n",
    "                \n",
    "                self.NV[case_pos[0]][case_pos[1]][case_pos[2]] += 1\n",
    "                self.V[case_pos[0]][case_pos[1]][case_pos[2]] += (G-self.V[case_pos[0]][case_pos[1]][case_pos[2]]) /self.NV[case_pos[0]][case_pos[1]][case_pos[2]]\n",
    "                \n",
    "            \n",
    "            \n",
    "    def generate_episode(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        states, rewards, actions = [], [], []\n",
    "        states.append([observation[1],observation[0],int(observation[2])])\n",
    "        \n",
    "        for t in range(100):\n",
    "            #self.print_observation(observation)\n",
    "            action = self.strategy(observation)\n",
    "            actions.append(action)\n",
    "            #print(\"Taking action: {}\".format( [\"Stick\", \"Hit\"][action]))\n",
    "            observation, reward, done, _ = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            states.append([observation[1],observation[0],int(observation[2])])\n",
    "            \n",
    "            if done:\n",
    "                #self.print_observation(observation)\n",
    "                #print(\"Game end. Reward: {}\\n\".format(float(reward)))\n",
    "                break\n",
    "        return states, rewards, actions\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38850fae-7bc1-4d80-9925-9bda428ec050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e8da4ca9804e82963256ac6bd73383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc491e9ae9346b3b1cdfce372ebe420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Model()\n",
    "m.aproximateV(100000)\n",
    "m1 = Model()\n",
    "m1.aproximateV(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f52943c-b1d6-45d8-bcc4-ec297377e05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129b2546aff74af7b34e3dc10c80e3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(12,22)\n",
    "Y = np.arange(1,11)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "ax.plot_surface(X, Y, m.V[:,:,0], linewidth=0, antialiased=False,cmap=cm.plasma)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2, projection='3d')\n",
    "ax.plot_surface(X, Y, m.V[:,:,1], linewidth=0, antialiased=False,cmap=cm.plasma)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "ax.plot_surface(X, Y, m1.V[:,:,0], linewidth=0, antialiased=False, cmap=cm.coolwarm)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "ax.plot_surface(X, Y, m1.V[:,:,1], linewidth=0, antialiased=False, cmap=cm.coolwarm)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44179e9-e625-4c9d-b3d3-2bbb528631b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
